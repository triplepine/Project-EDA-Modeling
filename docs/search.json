[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(corrplot)\nlibrary(caret)\nlibrary(Metrics)\nlibrary(ROSE)\n\n\nModeling\nOur goal is to use the diabetes data set to create models for predicting the Diabetes_binary variable (using caret). We will use logLoss as our metric to evaluate models. For all model types use logLoss with 5 fold cross-validation to select the best model.\nAbout logLoss\nLogarithmic Loss, also known as LogLoss, is a performance metric for evaluating the predictions of probabilities of binary (or multiclass) classification models. It measures the uncertainty of our predictions based on how much they diverge from the actual labels. Lower LogLoss values indicate better performance.\nWhy we prefer logLoss here to things like accuracy?\nWhile accuracy is a straightforward and intuitive metric, it has limitations in the context of probabilistic predictions and imbalanced datasets. Accuracy only considers whether the predicted class matches the actual class, without considering the confidence of the predictions. A model that predicts a low probability for the correct class is considered equally good as one that predicts 0.99. LogLoss penalizes predictions that are confident but wrong more heavily than those that are less confident. It provides a better measure of the quality of the probability estimates. In datasets with imbalanced classes, accuracy can be misleading. A model that always predicts the majority class can have high accuracy but poor performance in terms of identifying the minority class. LogLoss considers the probability of the minority class and provides a more informative evaluation.\n\n\nLoad the processed data\n\ndiabetes &lt;- readRDS(\"processed_diabetes.rds\")\n\n\nSplit the Data\nNow split the data into a training (70% of the data) and test set (30% of the data).\n\nset.seed(123)\nsplit &lt;- createDataPartition(y=diabetes$Diabetes_binary, p=0.7,list=FALSE)\ntrain &lt;-diabetes[split, ]\ntest &lt;-diabetes[-split, ]\n\n\n\nLogistic Regression Models\nLogistic Regression is a statistical method used for binary classification problems, where the response variable can take on one of two possible outcomes. It allows for response from non-normal distribution and can have both continuous and categorical predictors. In our case, the response variable (Diabetes_binary) indicates whether a person has diabetes (Yes) or does not have diabetes (No).\nFitting three Candidate Logistic Regression models\n\n# Set up cross-validation \ncontrol &lt;- trainControl(method = \"cv\", \n                        number = 5,\n                        classProbs = TRUE, \n                        summaryFunction = mnLogLoss\n                        )\n\nModel 1: Includes predictors: HighChol+ HeartDiseaseorAttack+ PhysActivity +GenHlth + BMI\n\n# Fit Model 1: \nset.seed(123)\nlogRegFit_1 &lt;- train(Diabetes_binary ~  HighChol+ HeartDiseaseorAttack+ PhysActivity +GenHlth + BMI ,\n                     data = train, \n                     method = \"glm\", \n                     family = \"binomial\",\n                     metric = \"logLoss\", \n                     trControl = control)\nlogRegFit_1\n\nGeneralized Linear Model \n\n177577 samples\n     5 predictor\n     2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3383185\n\n\nThis Logistic Regression Model 1 has a logLoss of 0.3383.\nModel 2: Includes predictors: HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ GenHlth, possibly important based on data analysis.\n\n# Fit Model 2: Includes predictors: HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ GenHlth\nset.seed(123)\nlogRegFit_2 &lt;- train(Diabetes_binary ~  HighBP+ HighChol+ HeartDiseaseorAttack+ BMI + GenHlth, \n                     data = train, \n                     method = \"glm\", \n                     family = \"binomial\",\n                     metric = \"logLoss\", \n                     preProcess = c(\"center\",\"scale\"),\n                     trControl = control)\nlogRegFit_2\n\nGeneralized Linear Model \n\n177577 samples\n     5 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3293504\n\n\nThe logLoss of this model 2 is 0.32935.\nModel 3: LASSO Logistic Regression\n\n# Define a tune grid for lambda values (regularization strength)\ntune_grid &lt;- expand.grid(alpha = 1,  # Lasso regression\n                         lambda = seq(0, 1, by = 0.1))\n\n# Fit logistic regression with Lasso regularization\nset.seed(123)\nlogRegLasso &lt;- train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ BMI + GenHlth,\n                     data = train, \n                     method = \"glmnet\", \n                     family = \"binomial\",\n                     metric = \"logLoss\", \n                     preProcess = c(\"center\",\"scale\"),\n                     trControl = control,\n                     tuneGrid = tune_grid)\n\nlogRegLasso\n\nglmnet \n\n177577 samples\n     5 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results across tuning parameters:\n\n  lambda  logLoss  \n  0.0     0.3293697\n  0.1     0.4037576\n  0.2     0.4037576\n  0.3     0.4037576\n  0.4     0.4037576\n  0.5     0.4037576\n  0.6     0.4037576\n  0.7     0.4037576\n  0.8     0.4037576\n  0.9     0.4037576\n  1.0     0.4037576\n\nTuning parameter 'alpha' was held constant at a value of 1\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were alpha = 1 and lambda = 0.\n\n\nObtained the best tuning parameter lambda is 0, alpha is 1 and logLoss is 0.32937.\nFrom the above three models, I’d choose the model 2 with predictors:HighBP, HighChol,HeartDiseaseorAttack,PhysActivity,GenHlth as the best of these three logistic regression models.\n\n\nClassification Tree\nA classification tree, also known as a decision tree, is a predictive modeling method used for classification tasks. It splits up the data into distinct regions based on certain criteria, This splitting process is applied recursively to each subset, creating new nodes, until certain stopping conditions is met. The goal is to make the subsets as homogeneous as possible concerning the target variable. Trees can capture non-linear relationships between features and also can handle both numerical and categorical data.\nNow, let’s fit a classification tree using rpart: tuning parameter is cp, use values 0, 0.001, 0.002…\n\nset.seed(123)\ncl_tree_fit &lt;-train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ BMI + GenHlth,\n                 data=train,\n                 method=\"rpart\",\n                 trControl=control,\n                 preProcess=c(\"center\",\"scale\"),\n                 tuneGrid=data.frame(cp=seq(0,0.1,0.001)))\ncl_tree_fit\n\nCART \n\n177577 samples\n     5 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.3382465\n  0.001  0.3577225\n  0.002  0.3577942\n  0.003  0.3577827\n  0.004  0.4037576\n  0.005  0.4037576\n  0.006  0.4037576\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n  0.011  0.4037576\n  0.012  0.4037576\n  0.013  0.4037576\n  0.014  0.4037576\n  0.015  0.4037576\n  0.016  0.4037576\n  0.017  0.4037576\n  0.018  0.4037576\n  0.019  0.4037576\n  0.020  0.4037576\n  0.021  0.4037576\n  0.022  0.4037576\n  0.023  0.4037576\n  0.024  0.4037576\n  0.025  0.4037576\n  0.026  0.4037576\n  0.027  0.4037576\n  0.028  0.4037576\n  0.029  0.4037576\n  0.030  0.4037576\n  0.031  0.4037576\n  0.032  0.4037576\n  0.033  0.4037576\n  0.034  0.4037576\n  0.035  0.4037576\n  0.036  0.4037576\n  0.037  0.4037576\n  0.038  0.4037576\n  0.039  0.4037576\n  0.040  0.4037576\n  0.041  0.4037576\n  0.042  0.4037576\n  0.043  0.4037576\n  0.044  0.4037576\n  0.045  0.4037576\n  0.046  0.4037576\n  0.047  0.4037576\n  0.048  0.4037576\n  0.049  0.4037576\n  0.050  0.4037576\n  0.051  0.4037576\n  0.052  0.4037576\n  0.053  0.4037576\n  0.054  0.4037576\n  0.055  0.4037576\n  0.056  0.4037576\n  0.057  0.4037576\n  0.058  0.4037576\n  0.059  0.4037576\n  0.060  0.4037576\n  0.061  0.4037576\n  0.062  0.4037576\n  0.063  0.4037576\n  0.064  0.4037576\n  0.065  0.4037576\n  0.066  0.4037576\n  0.067  0.4037576\n  0.068  0.4037576\n  0.069  0.4037576\n  0.070  0.4037576\n  0.071  0.4037576\n  0.072  0.4037576\n  0.073  0.4037576\n  0.074  0.4037576\n  0.075  0.4037576\n  0.076  0.4037576\n  0.077  0.4037576\n  0.078  0.4037576\n  0.079  0.4037576\n  0.080  0.4037576\n  0.081  0.4037576\n  0.082  0.4037576\n  0.083  0.4037576\n  0.084  0.4037576\n  0.085  0.4037576\n  0.086  0.4037576\n  0.087  0.4037576\n  0.088  0.4037576\n  0.089  0.4037576\n  0.090  0.4037576\n  0.091  0.4037576\n  0.092  0.4037576\n  0.093  0.4037576\n  0.094  0.4037576\n  0.095  0.4037576\n  0.096  0.4037576\n  0.097  0.4037576\n  0.098  0.4037576\n  0.099  0.4037576\n  0.100  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\nWe will select the Classification Tree with cp=0, which logLoss value is 0.3382.\n\n\nRandom Forest\nRandom Forest is an ensemble learning method that create multiple trees from bootstrap samples and use a random subset of predictors for each bootstrap sample fit and average the results. It Combines the strengths of multiple trees to reduce the variance, improve performance and accuracy compared to the basic individual tree.\n\n# Set up cross-validation\ncontrol_rf &lt;- trainControl(method = \"cv\", \n                        number = 3, \n                        classProbs = TRUE, \n                        summaryFunction = mnLogLoss\n                        )\n\n# Fit Random Forest\n\nset.seed(123)\nrf_fit &lt;-train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+GenHlth,\n                 data=train,\n                 method=\"rf\",\n                 trControl=control_rf,\n                 #preProcess=c(\"center\",\"scale\"),\n                 tuneGrid=data.frame(mtry=1:5),\n                 ntree=100)\nrf_fit\n\nRandom Forest \n\n177577 samples\n     5 predictor\n     2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 118384, 118386, 118384 \nResampling results across tuning parameters:\n\n  mtry  logLoss \n  1     4.453257\n  2     4.308882\n  3     4.310903\n  4     4.344799\n  5     4.338836\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 2."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "Setups\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(caret)\nlibrary(Metrics)"
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA",
    "section": "Introduction",
    "text": "Introduction\n\nAbout the Dataset\nThis dataset contains health indicators related to diabetes status.\nResponse variable:\nDiabetes_binary, indicating the presence or absence of diabetes. 1 for individuals diagnosed with diabetes and 0 for individuals without diabetes.\nPredictor Variables:\nRead in the dataset\n\ndiabetes &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNotice that all the variables are numeric, but most of them (except BMI, MentHlth, PhysHlth, Age) are actually factors with different levels. We need to convert them to factors.\n\n# List of variables to remain numeric\nnumeric_vars &lt;- c(\"BMI\", \"MentHlth\", \"PhysHlth\", \"Age\")\n\n# Convert all other variables to factors\ndiabetes &lt;- diabetes %&gt;%\n  mutate(across(-all_of(numeric_vars), as.factor))\n\n# Ensure the levels of categorical predictors are valid R variable names\nfor (col in names(diabetes)) {\n  if (is.factor(diabetes[[col]])) {\n    levels(diabetes[[col]]) &lt;- make.names(levels(diabetes[[col]]))\n  }\n}\n\n# change the factor level to No, Yes for response Diabetes_binary\ndiabetes &lt;- diabetes %&gt;%\n  mutate(Diabetes_binary = ifelse(Diabetes_binary == \"X0\", \"No\", \"Yes\")) %&gt;%\n  mutate(Diabetes_binary = as.factor(Diabetes_binary))\n\n# Verify the conversions\nstr(diabetes)\n\ntibble [253,680 × 22] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"X0\",\"X1\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"X0\",\"X1\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"X0\",\"X1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"X0\",\"X1\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"X0\",\"X1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"X0\",\"X1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"X0\",\"X1\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"X0\",\"X1\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"X0\",\"X1\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"X0\",\"X1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"X0\",\"X1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"X0\",\"X1\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Factor w/ 5 levels \"X1\",\"X2\",\"X3\",..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"X0\",\"X1\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"X0\",\"X1\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"X1\",\"X2\",\"X3\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Factor w/ 8 levels \"X1\",\"X2\",\"X3\",..: 3 1 8 6 4 8 7 4 1 3 ...\n\n# Save the processed data to a file\nsaveRDS(diabetes, \"processed_diabetes.rds\")\n\nCheck on missing values in the dataset\n\nsum(is.na(diabetes))\n\n[1] 0\n\n\nData set does not contain any missing values.\n\n\nSummaries/Explanatory Data Analyzes\n\nNumeric variables\n\n# Summary statistics for BMI, MentHlth and PhysHlth variables\nsummary(diabetes %&gt;% select(BMI, MentHlth,PhysHlth))\n\n      BMI           MentHlth         PhysHlth     \n Min.   :12.00   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:24.00   1st Qu.: 0.000   1st Qu.: 0.000  \n Median :27.00   Median : 0.000   Median : 0.000  \n Mean   :28.38   Mean   : 3.185   Mean   : 4.242  \n 3rd Qu.:31.00   3rd Qu.: 2.000   3rd Qu.: 3.000  \n Max.   :98.00   Max.   :30.000   Max.   :30.000  \n\n\nFrom the output above, the min BMI is 12 but the Max is 98. The number of days in the past 30days has mental health not good (MentHlth) vary from 0 as min to 30 as max. Same as PhysHlth which stands for number of days of physical health not good in the past 30 days.\nLet’s calculate and plot the correlation between the numerical variables.\n\nm &lt;- cor(select(diabetes, c('BMI','MentHlth','PhysHlth')))\ncorrplot(m,method='color',\n         order='alphabet',\n         diag=FALSE,\n         col=COL2('PiYG'))\n\n\n\n\n\n\n\n\nIt seems PhysHlth and MentHlth are moderately linear correlated, and also PhysHlth has some positive linear correlation with BMI.\n\n\nCategorical variables\nNow, let’s see Diabetes_binary in the context of BMI\n\n# Create a box plot\nggplot(diabetes, aes(x = Diabetes_binary, y = BMI, fill=Sex)) +\n  geom_boxplot() +\n  labs(x = \"Diabetes_binary\", y = \"BMI\", title = \"Box Plot of BMI by Diabetes Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs shown in the above boxplot, BMI does not vary much between different Diabetes_binary.\nLet’s see Diabetes_binary with highBP and highChol\n\n# Create a bar plot for Diabetes_binary and HighBP\nggplot(diabetes, aes(x = Diabetes_binary, fill = HighBP)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Diabetes_binary)\", fill = \"High BP\", y = \"Proportion\", title = \"Proportion of High BP by Diabetes Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Create a bar plot for Diabetes_binary and HighChol\nggplot(diabetes, aes(x = Diabetes_binary, fill = HighChol)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Diabetes (binary)\", fill = \"High Cholesterol\", y = \"Proportion\", title = \"Proportion of High Cholesterol by Diabetes Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the graphs plotted above, both with high HighChol and HighBP have a higher proportion of diabetes.\nLet’s look at Diabetes_binary for different level of PhysActivity, HvyAlcoholConsump, GenHlth respectively.\n\n# Frequency table for Diabetes_binary and PhysActivity\nphys_activity_table &lt;- table(\"PhysActivity\"=diabetes$PhysActivity, \"diabetes\"=diabetes$Diabetes_binary)\nphys_activity_table\n\n            diabetes\nPhysActivity     No    Yes\n          X0  48701  13059\n          X1 169633  22287\n\n# Frequency table for Diabetes_binary and HvyAlcoholConsump\nhvy_alcohol_table &lt;- table(\"HvyAlcoholConsump\"=diabetes$HvyAlcoholConsump, \"diabetes\"=diabetes$Diabetes_binary)\nhvy_alcohol_table\n\n                 diabetes\nHvyAlcoholConsump     No    Yes\n               X0 204910  34514\n               X1  13424    832\n\n# Frequency table for Diabetes_binary and GenHlth\ngen_health_table &lt;- table(\"GenHlth\"=diabetes$GenHlth, \"diabetes\"=diabetes$Diabetes_binary)\ngen_health_table\n\n       diabetes\nGenHlth    No   Yes\n     X1 44159  1140\n     X2 82703  6381\n     X3 62189 13457\n     X4 21780  9790\n     X5  7503  4578\n\n\nThe tables show that larger number of those who engage in physical activity do not have diabetes compared to those who do not engage in physical activity. This can indicate an inverse relationship between physical activity and the prevalence of diabetes.While heavy alcohol comsuption does not show relationship with diabetes.\nGenHlth has 5 levels , so for clear visualization, let’s plot the proportion along with the frequency table for better visionliaztion.\n\n# Frequency table for Diabetes_binary and GenHlth\ngen_health_table &lt;- table(\"GenHlth\" = diabetes$GenHlth, \"Diabetes_binary\" = diabetes$Diabetes_binary)\n\n# Calculate proportions\ngen_health_prop &lt;- prop.table(gen_health_table, 1)\n\n# Combine frequency and proportion tables\ngen_health_combined &lt;- cbind(gen_health_table, round(gen_health_prop, 4))\n\n# Add meaningful column names\ncolnames(gen_health_combined) &lt;- c(\"No_Count\", \"Yes_Count\", \"No_Proportion\", \"Yes_Proportion\")\n\n# Convert to data frame for better readability\ngen_health_combined_df &lt;- as.data.frame(gen_health_combined)\n\n# Print the combined table\ncat(\"Frequency and Proportion Table for Diabetes_binary and GenHlth\\n\")\n\nFrequency and Proportion Table for Diabetes_binary and GenHlth\n\nprint(gen_health_combined_df)\n\n   No_Count Yes_Count No_Proportion Yes_Proportion\nX1    44159      1140        0.9748         0.0252\nX2    82703      6381        0.9284         0.0716\nX3    62189     13457        0.8221         0.1779\nX4    21780      9790        0.6899         0.3101\nX5     7503      4578        0.6211         0.3789\n\n\nWe see that the diabetes proportion significantly increases as the level of GenHlth increases."
  },
  {
    "objectID": "EDA.html#click-here-for-the-modeling-page",
    "href": "EDA.html#click-here-for-the-modeling-page",
    "title": "EDA",
    "section": "Click here for the Modeling Page",
    "text": "Click here for the Modeling Page"
  }
]