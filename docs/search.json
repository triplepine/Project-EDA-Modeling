[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(corrplot)\nlibrary(caret)\nlibrary(Metrics)\n\n\nModeling\nOur goal is to use the diabetes data set to create models for predicting the Diabetes_binary variable (using caret). We will use logLoss as our metric to evaluate models. For all model types use logLoss with 5 fold cross-validation to select the best model.\nAbout logLoss\nLogarithmic Loss, also known as LogLoss, is a performance metric for evaluating the predictions of probabilities of binary (or multiclass) classification models. It measures the uncertainty of our predictions based on how much they diverge from the actual labels. Lower LogLoss values indicate better performance.\nWhy we prefer logLoss here to things like accuracy?\nWhile accuracy is a straightforward and intuitive metric, it has limitations in the context of probabilistic predictions and imbalanced datasets. Accuracy only considers whether the predicted class matches the actual class, without considering the confidence of the predictions. A model that predicts a low probability for the correct class is considered equally good as one that predicts 0.99. LogLoss penalizes predictions that are confident but wrong more heavily than those that are less confident. It provides a better measure of the quality of the probability estimates. In datasets with imbalanced classes, accuracy can be misleading. A model that always predicts the majority class can have high accuracy but poor performance in terms of identifying the minority class. LogLoss considers the probability of the minority class and provides a more informative evaluation.\n\n\nLoad the processed data\n\ndiabetes &lt;- readRDS(\"processed_diabetes.rds\")\n\n\nSplit the Data\nNow split the data into a training (70% of the data) and test set (30% of the data).\n\nset.seed(123)\nsplit &lt;- createDataPartition(y=diabetes$Diabetes_binary, p=0.7,list=FALSE)\ntrain &lt;-diabetes[split, ]\ntest &lt;-diabetes[-split, ]\n\n\n\nLogistic Regression Models\nLogistic Regression is a statistical method used for binary classification problems, where the response variable can take on one of two possible outcomes. It allows for response from non-normal distribution and can have both continuous and categorical predictors. In our case, the response variable (Diabetes_binary) indicates whether a person has diabetes (Yes) or does not have diabetes (No).\nFitting three Candidate Logistic Regression models\n\n# Set up cross-validation \ncontrol &lt;- trainControl(method = \"cv\", \n                        number = 5,\n                        classProbs = TRUE, \n                        summaryFunction = mnLogLoss\n                        )\n\nModel 1: Includes predictors: HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity +GenHlth+ MentHlth\n\n# Fit Model 1: \nset.seed(123)\nlogRegFit_1 &lt;- train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity + MentHlth + GenHlth,\n                     data = train, \n                     method = \"glm\", \n                     family = \"binomial\",\n                     metric = \"logLoss\", \n                     trControl = control)\nlogRegFit_1\n\nGeneralized Linear Model \n\n177577 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3362196\n\n\nThis Logistic Regression Model 1 has a logLoss of 0.33622.\nModel 2: Includes predictors: HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ MentHlth.\n\n# Fit Model 2: Includes predictors: HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ GenHlth\nset.seed(123)\nlogRegFit_2 &lt;- train(Diabetes_binary ~  HighBP+ HighChol+ HeartDiseaseorAttack+ GenHlth+ MentHlth, \n                     data = train, \n                     method = \"glm\", \n                     family = \"binomial\",\n                     metric = \"logLoss\", \n                     preProcess = c(\"center\",\"scale\"),\n                     trControl = control)\nlogRegFit_2\n\nGeneralized Linear Model \n\n177577 samples\n     5 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (8), scaled (8) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3366284\n\n\nThe logLoss of this model 2 is 0.3364.\nModel 3: LASSO Logistic Regression\n\n# Define a tune grid for lambda values (regularization strength)\ntune_grid &lt;- expand.grid(alpha = 1,  # Lasso regression\n                         lambda = seq(0, 1, by = 0.1))\n\n# Fit logistic regression with Lasso regularization\nset.seed(123)\nlogRegLasso &lt;- train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ MentHlth,\n                     data = train, \n                     method = \"glmnet\", \n                     family = \"binomial\",\n                     metric = \"logLoss\", \n                     preProcess = c(\"center\",\"scale\"),\n                     trControl = control,\n                     tuneGrid = tune_grid)\n\nlogRegLasso\n\nglmnet \n\n177577 samples\n     5 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results across tuning parameters:\n\n  lambda  logLoss  \n  0.0     0.3542885\n  0.1     0.4037576\n  0.2     0.4037576\n  0.3     0.4037576\n  0.4     0.4037576\n  0.5     0.4037576\n  0.6     0.4037576\n  0.7     0.4037576\n  0.8     0.4037576\n  0.9     0.4037576\n  1.0     0.4037576\n\nTuning parameter 'alpha' was held constant at a value of 1\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were alpha = 1 and lambda = 0.\n\n\nObtained the best tuning parameter lambda is 0, alpha is 1 and logLoss is 0.33624.\nFrom the above three models, I’d choose the model 1 with predictors:HighBP, HighChol,HeartDiseaseorAttack,PhysActivity,GenHlth, MentHlth as the best of these three logistic regression models.\n\n\nClassification Tree\nA classification tree, also known as a decision tree, is a predictive modeling method used for classification tasks. It splits up the data into distinct regions based on certain criteria, This splitting process is applied recursively to each subset, creating new nodes, until certain stopping conditions is met. The goal is to make the subsets as homogeneous as possible concerning the target variable. Trees can capture non-linear relationships between features and also can handle both numerical and categorical data.\nNow, let’s fit a classification tree using rpart: tuning parameter is cp, use values 0, 0.001, 0.002…\n\nset.seed(123)\ncl_tree_fit &lt;-train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ GenHlth + MentHlth,\n                 data=train,\n                 method=\"rpart\",\n                 trControl=control,\n                 preProcess=c(\"center\",\"scale\"),\n                 tuneGrid=data.frame(cp=seq(0,0.1,0.001)))\ncl_tree_fit\n\nCART \n\n177577 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (9), scaled (9) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.3418082\n  0.001  0.3675003\n  0.002  0.4037576\n  0.003  0.4037576\n  0.004  0.4037576\n  0.005  0.4037576\n  0.006  0.4037576\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n  0.011  0.4037576\n  0.012  0.4037576\n  0.013  0.4037576\n  0.014  0.4037576\n  0.015  0.4037576\n  0.016  0.4037576\n  0.017  0.4037576\n  0.018  0.4037576\n  0.019  0.4037576\n  0.020  0.4037576\n  0.021  0.4037576\n  0.022  0.4037576\n  0.023  0.4037576\n  0.024  0.4037576\n  0.025  0.4037576\n  0.026  0.4037576\n  0.027  0.4037576\n  0.028  0.4037576\n  0.029  0.4037576\n  0.030  0.4037576\n  0.031  0.4037576\n  0.032  0.4037576\n  0.033  0.4037576\n  0.034  0.4037576\n  0.035  0.4037576\n  0.036  0.4037576\n  0.037  0.4037576\n  0.038  0.4037576\n  0.039  0.4037576\n  0.040  0.4037576\n  0.041  0.4037576\n  0.042  0.4037576\n  0.043  0.4037576\n  0.044  0.4037576\n  0.045  0.4037576\n  0.046  0.4037576\n  0.047  0.4037576\n  0.048  0.4037576\n  0.049  0.4037576\n  0.050  0.4037576\n  0.051  0.4037576\n  0.052  0.4037576\n  0.053  0.4037576\n  0.054  0.4037576\n  0.055  0.4037576\n  0.056  0.4037576\n  0.057  0.4037576\n  0.058  0.4037576\n  0.059  0.4037576\n  0.060  0.4037576\n  0.061  0.4037576\n  0.062  0.4037576\n  0.063  0.4037576\n  0.064  0.4037576\n  0.065  0.4037576\n  0.066  0.4037576\n  0.067  0.4037576\n  0.068  0.4037576\n  0.069  0.4037576\n  0.070  0.4037576\n  0.071  0.4037576\n  0.072  0.4037576\n  0.073  0.4037576\n  0.074  0.4037576\n  0.075  0.4037576\n  0.076  0.4037576\n  0.077  0.4037576\n  0.078  0.4037576\n  0.079  0.4037576\n  0.080  0.4037576\n  0.081  0.4037576\n  0.082  0.4037576\n  0.083  0.4037576\n  0.084  0.4037576\n  0.085  0.4037576\n  0.086  0.4037576\n  0.087  0.4037576\n  0.088  0.4037576\n  0.089  0.4037576\n  0.090  0.4037576\n  0.091  0.4037576\n  0.092  0.4037576\n  0.093  0.4037576\n  0.094  0.4037576\n  0.095  0.4037576\n  0.096  0.4037576\n  0.097  0.4037576\n  0.098  0.4037576\n  0.099  0.4037576\n  0.100  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\nWe will select the Classification Tree with cp=0, the corresponding logLoss value is 0.3418.\n\n\nRandom Forest\nRandom Forest is an ensemble learning method that create multiple trees from bootstrap samples and use a random subset of predictors for each bootstrap sample fit and average the results. It Combines the strengths of multiple trees to reduce the variance, improve performance and accuracy compared to the basic individual tree.\n\n# Set up cross-validation\ncontrol_rf &lt;- trainControl(method = \"cv\", \n                        number = 3, \n                        classProbs = TRUE, \n                        summaryFunction = mnLogLoss\n                        )\n\n# Fit Random Forest\n\nset.seed(123)\nrf_fit &lt;-train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+GenHlth + MentHlth,\n                 data=train,\n                 method=\"rf\",\n                 trControl=control_rf,\n                 #preProcess=c(\"center\",\"scale\"),\n                 tuneGrid=data.frame(mtry=1:6),\n                 ntree=100)\nrf_fit\n\nRandom Forest \n\n177577 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 118384, 118386, 118384 \nResampling results across tuning parameters:\n\n  mtry  logLoss \n  1     4.437618\n  2     4.147667\n  3     4.094074\n  4     4.048506\n  5     4.004992\n  6     3.927258\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 6.\n\n\nSo we will pick the random forest model with mtry = 6 (note that the corresponding logLoss is 3.927, which seems unreasonable, but for comparison purposes, we will still select this one as it’s the best among this type).\n\n\nFinal Model Selection\nNow, we have chosen the best model of each type. We will compare the three models on the test set and find the overall best model.\nLogistic Regression Predictions and logloss calculation\n\n# Ensure all required predictors are included in the newdata\nrequired_predictors &lt;- c(\"HighBP\", \"HighChol\", \"HeartDiseaseorAttack\", \"PhysActivity\", \"GenHlth\", \"MentHlth\")\n\n# Predict on the test set using all required predictors\nfitted_log &lt;- predict(logRegFit_1, newdata = test %&gt;% select(all_of(required_predictors)), type = \"prob\")\n\n# Convert the predicted probabilities to numeric format for mnLogLoss\npred_data &lt;- data.frame(obs = test$Diabetes_binary, `No` = fitted_log[, 1], `Yes` = fitted_log[, 2])\n\n# Check the structure of pred_data to ensure it matches expected format\nstr(pred_data)\n\n'data.frame':   76103 obs. of  3 variables:\n $ obs: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 2 2 1 1 2 2 ...\n $ No : num  0.931 0.914 0.852 0.849 0.459 ...\n $ Yes: num  0.0692 0.0857 0.1479 0.151 0.5407 ...\n\n# Calculate log loss using the mnLogLoss function from caret\nlog_loss_logRegFit_1 &lt;- mnLogLoss(pred_data, lev = levels(test$Diabetes_binary))\nprint(log_loss_logRegFit_1)\n\n  logLoss \n0.3306129 \n\n\nClassical Tree Predictions and logloss calculation\n\n# Ensure all required predictors are included in the newdata\nrequired_predictors &lt;- c(\"HighBP\", \"HighChol\", \"HeartDiseaseorAttack\", \"PhysActivity\", \"GenHlth\", \"MentHlth\")\n\n# Predict on the test set using all required predictors\nfitted_cl_tree &lt;- predict(cl_tree_fit, newdata = test %&gt;% select(all_of(required_predictors)), type = \"prob\")\n\n# Convert the predicted probabilities to numeric format for mnLogLoss\npred_data &lt;- data.frame(obs = test$Diabetes_binary, `No` = fitted_cl_tree[, 1], `Yes` = fitted_cl_tree[, 2])\n\n# Check the structure of pred_data to ensure it matches expected format\nstr(pred_data)\n\n'data.frame':   76103 obs. of  3 variables:\n $ obs: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 2 2 1 1 2 2 ...\n $ No : num  0.903 0.899 0.842 0.842 0.406 ...\n $ Yes: num  0.0965 0.1008 0.1584 0.1584 0.5938 ...\n\n# Calculate log loss using the mnLogLoss function from caret\nlog_loss_cl_tree &lt;- mnLogLoss(pred_data, lev = levels(test$Diabetes_binary))\nprint(log_loss_cl_tree)\n\n  logLoss \n0.3361602 \n\n\nRandom Forest Model Predictions and log loss calculation\n\n# Predict on the test set using all required predictors\nfitted_rf &lt;- predict(rf_fit, newdata = test %&gt;% select(all_of(required_predictors)), type = \"prob\")\n\n# Convert the predicted probabilities to numeric format for mnLogLoss\npred_data &lt;- data.frame(obs = test$Diabetes_binary, `No` = fitted_rf[, 1], `Yes` = fitted_rf[, 2])\n\n# Check the structure of pred_data to ensure it matches expected format\nstr(pred_data)\n\n'data.frame':   76103 obs. of  3 variables:\n $ obs: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 2 2 1 1 2 2 ...\n $ No : num  1 1 1 1 0.01 1 1 1 1 0.1 ...\n $ Yes: num  0 0 0 0 0.99 0 0 0 0 0.9 ...\n\n# Calculate log loss using the mnLogLoss function from caret\nlog_loss_rf &lt;- mnLogLoss(pred_data, lev = levels(test$Diabetes_binary))\nprint(log_loss_rf)\n\n logLoss \n3.951543 \n\n\nBased on the logLoss metric comparison, we will select the logistic regression model with logLoss = 0.3306 as the best model.\n\n# Save the best model to a file\nsaveRDS(logRegFit_1, \"logRegFit_1.rds\")"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "Setups\n\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(caret)\nlibrary(Metrics)\n\n\nIntroduction\n\nAbout the Dataset\nThis dataset contains health indicators related to diabetes status.\nResponse variable:\nDiabetes_binary, indicating the presence or absence of diabetes. “Yes” for individuals diagnosed with diabetes and “No” for individuals without diabetes.\nPredictor Variables (of interest):\n\nHighBP (High Blood Pressure):\nCategorical (Factor): 0 = no high BP 1 = high BP\nHighChol (High Cholesterol):\nCategorical (Factor): 0 = no high cholesterol 1 = high cholesterol\nBMI (Body Mass Index):\nContinuous (Numeric): Body Mass Index\nHeartDiseaseorAttack (Heart Disease):\nCategorical (Factor): coronary heart disease (CHD) or myocardial infarction (MI) 0 = no 1 = yes\nPhysActivity (Physical Activity):\nCategorical (Factor) : physical activity in past 30 days - not including job 0 = no 1 = yes\nHvyAlcoholConsump (Heavy Alcohol Consumption):\nCategorical (Factor): (adult men &gt;=14 drinks per week and adult women&gt;=7 drinks per week) 0 = no 1 = yes\nGenHlth (General Health):\nCategorical (Factor): Would you say that in general your health is: scale 1-5 1 = excellent 2 = very good 3 = good 4 = fair 5 = poor\nMentHlth (Mental Health):\nContinuous (Numeric): days of poor mental health scale 1-30 days\nPhysHlth (Physical Health):\nContinuous (Numeric): physical illness or injury days in past 30 days scale 1-30\n\n\n\nGoal\nThe goal is to comprehensively understand my data set, determine which predictors might be the most significant contributors to diabetes risk via EDA, and develop a predictive model that can accurately identify individuals at risk of developing diabetes based on their health and lifestyle factors.\n\n\n\nExploratory Data Analysis\nRead in the dataset\n\ndiabetes &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNotice that all the original variables are numeric, but most of them (except BMI, MentHlth, PhysHlth) are actually factors with different levels. We need to convert them to factors.\n\n# List of variables to remain numeric\nnumeric_vars &lt;- c(\"BMI\", \"MentHlth\", \"PhysHlth\")\n\n# Convert all other variables to factors\ndiabetes &lt;- diabetes %&gt;%\n  mutate(across(-all_of(numeric_vars), as.factor))\n\n# Convert variables of interest to factors with meaningful level names\ndiabetes &lt;- diabetes %&gt;%\n  mutate(\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, levels = c(1, 2, 3, 4, 5), labels = c(\"Excellent\", \"Very Good\", \"Good\", \"Fair\", \"Poor\")),\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0,1) , labels= c(\"No\", \"Yes\")),\n    Diabetes_num = as.numeric(Diabetes_binary) # for logLoss calculation\n  )\n\n# Verify the conversions\nstr(diabetes)\n\ntibble [253,680 × 23] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary     : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"0\",\"1\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"0\",\"1\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Factor w/ 5 levels \"Excellent\",\"Very Good\",..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : Factor w/ 13 levels \"1\",\"2\",\"3\",\"4\",..: 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Factor w/ 8 levels \"1\",\"2\",\"3\",\"4\",..: 3 1 8 6 4 8 7 4 1 3 ...\n $ Diabetes_num        : num [1:253680] 1 1 1 1 1 1 1 1 2 1 ...\n\n# Save the processed data to a file\nsaveRDS(diabetes, \"processed_diabetes.rds\")\n\nCheck on missing values in the data set\n\nsum(is.na(diabetes))\n\n[1] 0\n\n\nData set does not contain any missing values.\n\nSummaries/Explanatory Data Analyzes\n\nNumeric variables\n\n# Summary statistics for BMI, MentHlth and PhysHlth variables\nsummary(diabetes %&gt;% select(BMI, MentHlth,PhysHlth))\n\n      BMI           MentHlth         PhysHlth     \n Min.   :12.00   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:24.00   1st Qu.: 0.000   1st Qu.: 0.000  \n Median :27.00   Median : 0.000   Median : 0.000  \n Mean   :28.38   Mean   : 3.185   Mean   : 4.242  \n 3rd Qu.:31.00   3rd Qu.: 2.000   3rd Qu.: 3.000  \n Max.   :98.00   Max.   :30.000   Max.   :30.000  \n\n\nFrom the output above, the min BMI is 12 but the Max is 98. The number of days in the past 30days has mental health not good (MentHlth) vary from 0 as min to 30 as max. Same as PhysHlth which stands for number of days of physical health not good in the past 30 days.\nLet’s calculate and plot the correlation between the numerical variables.\n\nm &lt;- cor(select(diabetes, c('BMI','MentHlth','PhysHlth')))\ncorrplot(m,method='color',\n         order='alphabet',\n         diag=FALSE,\n         col=COL2('PiYG'))\n\n\n\n\n\n\n\n\nIt seems PhysHlth and MentHlth are moderately linear correlated, and also PhysHlth has some positive linear correlation with BMI.\n\n# Create a density plot to explore the relationship between numeric MentHlth and Diabetes_binary\nggplot(diabetes, aes(x = MentHlth, fill = Diabetes_binary)) +\n  geom_density(alpha = 0.6) +\n  labs(x = \"Mental Health Number in 30 days\", y = \"Density\", title = \"Density Plot of Mental Health by Diabetes Status\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"No\" = \"blue\", \"Yes\" = \"red\"))\n\n\n\n\n\n\n\n\nIt shows that people diagonozed with diabetes have more mental health issues in 30 days.\n\n# Create a density plot to explore the relationship between numeric PhysHlth and Diabetes_binary\nggplot(diabetes, aes(x = PhysHlth, fill = Diabetes_binary)) +\n  geom_density(alpha = 0.6) +\n  labs(x = \"Physical Health Number in 30 days\", y = \"Density\", title = \"Density Plot of Mental Health by Diabetes Status\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"No\" = \"blue\", \"Yes\" = \"red\"))\n\n\n\n\n\n\n\n\nThere is no evidence that people diagonozed diabetes have more physical health issues within 30 days.\nNow, let’s see Diabetes_binary in the context of BMI\n\n# Create a box plot to explore the relationship between Diabetes_binary and BMI\nggplot(diabetes, aes(x = Diabetes_binary, y = BMI)) +\n  geom_boxplot(fill = c(\"blue\", \"red\")) +\n  labs(x = \"Diabetes Status\", y = \"BMI\", title = \"Box Plot of BMI by Diabetes Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs shown in the above boxplot, BMI does not vary much between different Diabetes_binary level.\n\n\nCategorical variables\nLet’s see Diabetes_binary with highBP and highChol.\n\n# Create a bar plot for Diabetes_binary and HighBP\nggplot(diabetes, aes(x = Diabetes_binary, fill = HighBP)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Diabetes_binary)\", fill = \"High BP\", y = \"Proportion\", title = \"Proportion of High BP by Diabetes Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Create a bar plot for Diabetes_binary and HighChol\nggplot(diabetes, aes(x = Diabetes_binary, fill = HighChol)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Diabetes (binary)\", fill = \"High Cholesterol\", y = \"Proportion\", title = \"Proportion of High Cholesterol by Diabetes Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the graphs plotted above, both with high HighChol and HighBP have a higher proportion of diabetes.\nLet’s check the relationship between HeartDiseaseorAttack and Diabetes_binary\n\n# Create a bar plot to explore the relationship between HeartDiseaseorAttack and Diabetes_binary (Counts)\ncount_plot &lt;- ggplot(diabetes, aes(x = HeartDiseaseorAttack, fill = Diabetes_binary)) +\n  geom_bar(position = \"dodge\") +\n  labs(x = \"Heart Disease or Heart Attack\", y = \"Count\", title = \"Count of Heart Disease or Heart Attack by Diabetes Status\", fill = \"Diabetes Status\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"No\" = \"blue\", \"Yes\" = \"red\"))\ncount_plot\n\n\n\n\n\n\n\n# Create a bar plot to explore the relationship between HeartDiseaseorAttack and Diabetes_binary (Proportions)\nproportion_plot &lt;- ggplot(diabetes, aes(x = HeartDiseaseorAttack, fill = Diabetes_binary)) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Heart Disease or Heart Attack\", y = \"Proportion\", title = \"Proportion of Heart Disease or Heart Attack by Diabetes Status\", fill = \"Diabetes Status\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"No\" = \"blue\", \"Yes\" = \"red\"))\n\nproportion_plot\n\n\n\n\n\n\n\n\nIt looks like the proportion of HeartDiseaseorAttack in person with diabetes are much higher than that without heart disease or heart attack.\nLet’s look at Diabetes_binary for different level of PhysActivity, HvyAlcoholConsump, GenHlth and Fruits respectively.\n\n# Frequency table for Diabetes_binary and PhysActivity\nphys_activity_table &lt;- table(\"PhysActivity\"=diabetes$PhysActivity, \"diabetes\"=diabetes$Diabetes_binary)\nphys_activity_table\n\n            diabetes\nPhysActivity     No    Yes\n         No   48701  13059\n         Yes 169633  22287\n\n\nThe tables show that larger number of those who engage in physical activity do not have diabetes compared to those who do not engage in physical activity. This can indicate an inverse relationship between physical activity and the prevalence of diabetes.\n\n# Frequency table for Diabetes_binary and HvyAlcoholConsump\nhvy_alcohol_table &lt;- table(\"HvyAlcoholConsump\"=diabetes$HvyAlcoholConsump, \"diabetes\"=diabetes$Diabetes_binary)\nhvy_alcohol_table\n\n                 diabetes\nHvyAlcoholConsump     No    Yes\n              No  204910  34514\n              Yes  13424    832\n\n\nThe heavy alcohol consumption does not show relationship with diabetes.\n\n# Frequency table for Diabetes_binary and have Fruits or not once a day\nft_table &lt;- table(\"Fruits\"=diabetes$Fruits, \"diabetes\"=diabetes$Diabetes_binary)\nft_table\n\n      diabetes\nFruits     No    Yes\n     0  78129  14653\n     1 140205  20693\n\n\nIt doesn’t seem the fruit affect the Diabetes_binary significantly.\nGenHlth has 5 levels , so for clear visualization, let’s plot the proportion along with the frequency for better visionlization.\n\n# Create the ggplot for proportions and frequencies\nggplot(data = diabetes, aes(x = GenHlth, fill = Diabetes_binary)) +\n  geom_bar(position = \"fill\", stat = \"count\") +\n  scale_y_continuous(labels = scales::percent) +\n  geom_text(stat = \"count\", aes(label = ..count..), position = position_fill(vjust = 0.5)) +\n  labs(title = \"Proportion of Diabetes by General Health\",\n       x = \"General Health\",\n       y = \"Proportion\",\n       fill = \"Diabetes\") +\n  scale_fill_manual(values = c(\"No\" = \"pink\", \"Yes\" = \"lightblue\")) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\nWe see that the diabetes proportion significantly increases as the level of GenHlth increases (health condition gets poorer).\nOverall, from the EDA we will focus on the predictors HighBP, HighChol, HeartDiseaseorAttack, PhysActivity, MentHlth, GenHlth.\n\n\n\n\nClick here for the Modeling Page"
  }
]