---
title: "Modeling"
format: html
editor: visual
---

```{r setup, warning=FALSE, message=FALSE}
library(tidyverse)
library(corrplot)
library(caret)
library(Metrics)

```

### Modeling

Our goal is to use the diabetes data set to create models for predicting the Diabetes_binary variable (using caret). We will use logLoss as our metric to evaluate models. For all model types use logLoss with 5 fold cross-validation to select the best model.

About logLoss

Logarithmic Loss, also known as LogLoss, is a performance metric for evaluating the predictions of probabilities of binary (or multiclass) classification models. It measures the uncertainty of our predictions based on how much they diverge from the actual labels. Lower LogLoss values indicate better performance.

Why we prefer logLoss here to things like accuracy?

While accuracy is a straightforward and intuitive metric, it has limitations in the context of probabilistic predictions and imbalanced datasets. Accuracy only considers whether the predicted class matches the actual class, without considering the confidence of the predictions. A model that predicts a low probability for the correct class is considered equally good as one that predicts 0.99. LogLoss penalizes predictions that are confident but wrong more heavily than those that are less confident. It provides a better measure of the quality of the probability estimates. In datasets with imbalanced classes, accuracy can be misleading. A model that always predicts the majority class can have high accuracy but poor performance in terms of identifying the minority class. LogLoss considers the probability of the minority class and provides a more informative evaluation.

# Load the processed data
```{r}
diabetes <- readRDS("processed_diabetes.rds")
```

#### Split the Data

Now split the data into a training (70% of the data) and test set (30% of the data).

```{r}
set.seed(123)
split <- createDataPartition(y=diabetes$Diabetes_binary, p=0.7,list=FALSE)
train <-diabetes[split, ]
test <-diabetes[-split, ]
```

#### Logistic Regression Models

Logistic Regression is a statistical method used for binary classification problems, where the response variable can take on one of two possible outcomes. It allows for response from non-normal distribution and can have both continuous and categorical predictors. In our case, the response variable (Diabetes_binary) indicates whether a person has diabetes (Yes) or does not have diabetes (No).

Fitting three Candidate Logistic Regression models

```{r}
# Set up cross-validation 
control <- trainControl(method = "cv", 
                        number = 5,
                        classProbs = TRUE, 
                        summaryFunction = mnLogLoss
                        )
```

Model 1: Includes predictors: HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity +GenHlth+ MentHlth 

```{r}
# Fit Model 1: 
set.seed(123)
logRegFit_1 <- train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity +GenHlth+ MentHlth ,
                     data = train, 
                     method = "glm", 
                     family = "binomial",
                     metric = "logLoss", 
                     trControl = control)
logRegFit_1
```

This Logistic Regression Model 1 has a logLoss of 0.33622.

Model 2: Includes predictors:  HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ GenHlth.

```{r}
# Fit Model 2: Includes predictors: HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ GenHlth
set.seed(123)
logRegFit_2 <- train(Diabetes_binary ~  HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ GenHlth +MentHlth, 
                     data = train, 
                     method = "glm", 
                     family = "binomial",
                     metric = "logLoss", 
                     preProcess = c("center","scale"),
                     trControl = control)
logRegFit_2
```
The logLoss of this model 2 is 0.3364.

Model 3: LASSO Logistic Regression

```{r}
# Define a tune grid for lambda values (regularization strength)
tune_grid <- expand.grid(alpha = 1,  # Lasso regression
                         lambda = seq(0, 1, by = 0.1))

# Fit logistic regression with Lasso regularization
set.seed(123)
logRegLasso <- train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ GenHlth + MentHlth,
                     data = train, 
                     method = "glmnet", 
                     family = "binomial",
                     metric = "logLoss", 
                     preProcess = c("center","scale"),
                     trControl = control,
                     tuneGrid = tune_grid)

logRegLasso
```

Obtained the best tuning parameter lambda is 0, alpha is 1 and logLoss is 0.33624.

From the above three models, I'd choose the model 1 with predictors:HighBP, HighChol,HeartDiseaseorAttack,PhysActivity,GenHlth, MentHlth as the best of these three logistic regression models.

#### Classification Tree

A classification tree, also known as a decision tree, is a predictive modeling method used for classification tasks. It splits up the data into distinct regions based on certain criteria, This splitting process is applied recursively to each subset, creating new nodes, until certain stopping conditions is met. The goal is to make the subsets as homogeneous as possible concerning the target variable. Trees can capture non-linear relationships between features and also can handle both numerical and categorical data.

Now, let's fit a classification tree using rpart: tuning parameter is cp, use values 0, 0.001, 0.002...

```{r, warning=FALSE, message=FALSE}
set.seed(123)
cl_tree_fit <-train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ GenHlth + MentHlth,
                 data=train,
                 method="rpart",
                 trControl=control,
                 preProcess=c("center","scale"),
                 tuneGrid=data.frame(cp=seq(0,0.1,0.001)))
cl_tree_fit
```

We will select the Classification Tree with cp=0, the corresponding logLoss value is 0.3418.

#### Random Forest

Random Forest is an ensemble learning method that create multiple trees from bootstrap samples and use a random subset of predictors for each bootstrap sample fit and average the results. It Combines the strengths of multiple trees to reduce the variance, improve performance and accuracy compared to the basic individual tree.

```{r, warning=FALSE, message=FALSE}
# Set up cross-validation
control_rf <- trainControl(method = "cv", 
                        number = 3, 
                        classProbs = TRUE, 
                        summaryFunction = mnLogLoss
                        )

# Fit Random Forest

set.seed(123)
rf_fit <-train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+GenHlth + MentHlth,
                 data=train,
                 method="rf",
                 trControl=control_rf,
                 #preProcess=c("center","scale"),
                 tuneGrid=data.frame(mtry=1:6),
                 ntree=100)
rf_fit

```
So we will pick the random forest model with mtry = 6 (note that the corresponding logLoss is 3.927, which seems unreasonable, but for comparison purposes, we will still select this one as it's the best among this type).


#### Final Model Selection

Now, we have chosen the best model of each type. We will compare the three models on the test set and find the overall best model.


Logistic Regression Predictions and logloss calculation
```{r}
fitted_log <- predict(logRegFit_1, newdata = select(test,- c(Diabetes_binary, Diabetes_num)), type = "prob")[ ,2]

log_loss_logRegFit_1 <- logLoss(test$Diabetes_num, fitted_log)
log_loss_logRegFit_1
```

Classical Tree Predictions and logloss calculation
```{r}
fitted_cl <- predict(cl_tree_fit, newdata = select(test,- c(Diabetes_binary, Diabetes_num)), type = "prob")[, 2]

# Add smoothing to calibrated probabilities
epsilon <- 1e-15
fitted_cl <- pmin(pmax(fitted_cl, epsilon), 1 - epsilon)
log_loss_class <- logLoss(test$Diabetes_num, fitted_cl)
log_loss_class
```

Random Forest Model Predictions and log loss calculation

```{r}
fitted_rf <- predict(rf_fit, newdata = select(test, -c(Diabetes_binary, Diabetes_num)), type = "prob")[, 2]
epsilon <- 1e-15
fitted_rf <- pmin(pmax(fitted_rf, epsilon), 1 - epsilon)
log_loss_rf <- logLoss(test$Diabetes_num, fitted_rf)
log_loss_rf
```


Based on the logLoss metric comparison, we will select the classfication tree as the best model.

```{r}
# Save the best model to a file
saveRDS(cl_tree_fit, "cl_tree_fit.rds")
```

