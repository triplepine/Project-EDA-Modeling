---
title: "Modeling"
format: html
editor: visual
---

```{r setup, warning=FALSE, message=FALSE}
library(tidyverse)
library(corrplot)
library(caret)
library(Metrics)

```

### Modeling

Our goal is to use the diabetes data set to create models for predicting the Diabetes_binary variable (using caret). We will use logLoss as our metric to evaluate models. For all model types use logLoss with 5 fold cross-validation to select the best model.

About logLoss

Logarithmic Loss, also known as LogLoss, is a performance metric for evaluating the predictions of probabilities of binary (or multiclass) classification models. It measures the uncertainty of our predictions based on how much they diverge from the actual labels. Lower LogLoss values indicate better performance.

Why we prefer logLoss here to things like accuracy?

While accuracy is a straightforward and intuitive metric, it has limitations in the context of probabilistic predictions and imbalanced datasets. Accuracy only considers whether the predicted class matches the actual class, without considering the confidence of the predictions. A model that predicts a low probability for the correct class is considered equally good as one that predicts 0.99. LogLoss penalizes predictions that are confident but wrong more heavily than those that are less confident. It provides a better measure of the quality of the probability estimates. In datasets with imbalanced classes, accuracy can be misleading. A model that always predicts the majority class can have high accuracy but poor performance in terms of identifying the minority class. LogLoss considers the probability of the minority class and provides a more informative evaluation.

# Load the processed data
```{r}
diabetes <- readRDS("processed_diabetes.rds")
```

#### Split the Data

Now split the data into a training (70% of the data) and test set (30% of the data).

```{r}
set.seed(123)
split <- createDataPartition(y=diabetes$Diabetes_binary, p=0.7,list=FALSE)
train <-diabetes[split, ]
test <-diabetes[-split, ]
```

#### Logistic Regression Models

Logistic Regression is a statistical method used for binary classification problems, where the response variable can take on one of two possible outcomes. It allows for response from non-normal distribution and can have both continuous and categorical predictors. In our case, the response variable (Diabetes_binary) indicates whether a person has diabetes (Yes) or does not have diabetes (No).

Fitting three Candidate Logistic Regression models

```{r}
# Set up cross-validation 
control <- trainControl(method = "cv", 
                        number = 5,
                        classProbs = TRUE, 
                        summaryFunction = mnLogLoss
                        )
```

Model 1: Includes predictors: HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity +GenHlth+ MentHlth 

```{r}
# Fit Model 1: 
set.seed(123)
logRegFit_1 <- train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity + MentHlth + GenHlth,
                     data = train, 
                     method = "glm", 
                     family = "binomial",
                     metric = "logLoss", 
                     trControl = control)

summary(logRegFit_1)
logRegFit_1
```

This Logistic Regression Model 1 has a logLoss of 0.33622.


Model 2: Includes predictors:  HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ MentHlth.

```{r}
# Fit Model 2: Includes predictors: HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ GenHlth + MentHlth
set.seed(123)
logRegFit_2 <- train(Diabetes_binary ~  HighBP+ HighChol+ HeartDiseaseorAttack+ GenHlth+ MentHlth, 
                     data = train, 
                     method = "glm", 
                     family = "binomial",
                     metric = "logLoss", 
                     preProcess = c("center","scale"),
                     trControl = control)

summary(logRegFit_2)
logRegFit_2
```
The logLoss of this model 2 is 0.3366284.

Model 3: Logistic Regression including interaction term MentHlth: PhysHlth
```{r}
# Model 3 with interaction term 
set.seed(123)
logRegFit_3 <- train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity + MentHlth:PhysHlth + GenHlth,
                     data = train, 
                     method = "glm", 
                     family = "binomial",
                     metric = "logLoss", 
                     trControl = control)
summary(logRegFit_3)
logRegFit_3
```


From the above three models, I'd choose the model 1 with predictors: HighBP, HighChol, HeartDiseaseorAttack, PhysActivity, GenHlth, MentHlth with the lowest logLoss as the best of these three logistic regression models.


#### Classification Tree

A classification tree, also known as a decision tree, is a predictive modeling method used for classification tasks. It splits up the data into distinct regions based on certain criteria, This splitting process is applied recursively to each subset, creating new nodes, until certain stopping conditions is met. The goal is to make the subsets as homogeneous as possible concerning the target variable. Trees can capture non-linear relationships between features and also can handle both numerical and categorical data.

Now, let's fit a classification tree using rpart: tuning parameter is cp, use values 0, 0.001, 0.002...

```{r, warning=FALSE, message=FALSE}
set.seed(123)
cl_tree_fit <-train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+ GenHlth + MentHlth,
                 data=train,
                 method="rpart",
                 trControl=control,
                 preProcess=c("center","scale"),
                 tuneGrid=data.frame(cp=seq(0,0.1,0.001)))
cl_tree_fit
```

We will select the Classification Tree with cp=0, the corresponding logLoss value is 0.3418.

#### Random Forest

Random Forest is an ensemble learning method that create multiple trees from bootstrap samples and use a random subset of predictors for each bootstrap sample fit and average the results. It Combines the strengths of multiple trees to reduce the variance, improve performance and accuracy compared to the basic individual tree.

```{r, warning=FALSE, message=FALSE}
# Set up cross-validation
control_rf <- trainControl(method = "cv", 
                        number = 3, 
                        classProbs = TRUE, 
                        summaryFunction = mnLogLoss
                        )

# Fit Random Forest

set.seed(123)
rf_fit <-train(Diabetes_binary ~ HighBP+ HighChol+ HeartDiseaseorAttack+ PhysActivity+GenHlth + MentHlth,
                 data=train,
                 method="rf",
                 trControl=control_rf,
                 #preProcess=c("center","scale"),
                 tuneGrid=data.frame(mtry=1:6),
                 ntree=100)
rf_fit

```
So we will pick the random forest model with mtry = 6 (note that the corresponding logLoss is 3.927, which seems unreasonable, but for comparison purposes, we will still select this one as it's the best among this type).


#### Final Model Selection

Now, we have chosen the best model of each type. We will compare the three models on the test set and find the overall best model.


Logistic Regression Predictions and logloss calculation

```{r}
# Ensure all required predictors are included in the newdata
required_predictors <- c("HighBP", "HighChol", "HeartDiseaseorAttack", "PhysActivity", "GenHlth", "MentHlth")

# Predict on the test set using all required predictors
fitted_log <- predict(logRegFit_1, newdata = test %>% select(all_of(required_predictors)), type = "prob")

# Convert the predicted probabilities to numeric format for mnLogLoss
pred_data <- data.frame(obs = test$Diabetes_binary, `No` = fitted_log[, 1], `Yes` = fitted_log[, 2])

# Check the structure of pred_data to ensure it matches expected format
str(pred_data)

# Calculate log loss using the mnLogLoss function from caret
log_loss_logRegFit_1 <- mnLogLoss(pred_data, lev = levels(test$Diabetes_binary))
print(log_loss_logRegFit_1)
```


Classical Tree Predictions and logloss calculation
```{r}
# Ensure all required predictors are included in the newdata
required_predictors <- c("HighBP", "HighChol", "HeartDiseaseorAttack", "PhysActivity", "GenHlth", "MentHlth")

# Predict on the test set using all required predictors
fitted_cl_tree <- predict(cl_tree_fit, newdata = test %>% select(all_of(required_predictors)), type = "prob")

# Convert the predicted probabilities to numeric format for mnLogLoss
pred_data <- data.frame(obs = test$Diabetes_binary, `No` = fitted_cl_tree[, 1], `Yes` = fitted_cl_tree[, 2])

# Check the structure of pred_data to ensure it matches expected format
str(pred_data)

# Calculate log loss using the mnLogLoss function from caret
log_loss_cl_tree <- mnLogLoss(pred_data, lev = levels(test$Diabetes_binary))
print(log_loss_cl_tree)
```


Random Forest Model Predictions and log loss calculation

```{r}
# Predict on the test set using all required predictors
fitted_rf <- predict(rf_fit, newdata = test %>% select(all_of(required_predictors)), type = "prob")

# Convert the predicted probabilities to numeric format for mnLogLoss
pred_data <- data.frame(obs = test$Diabetes_binary, `No` = fitted_rf[, 1], `Yes` = fitted_rf[, 2])

# Check the structure of pred_data to ensure it matches expected format
str(pred_data)

# Calculate log loss using the mnLogLoss function from caret
log_loss_rf <- mnLogLoss(pred_data, lev = levels(test$Diabetes_binary))
print(log_loss_rf)
```


Based on the logLoss metric comparison, we will select the logistic regression model 1 (logRegFit_1 with 6 predictors: HighBP+ HighChol+ HeartDiseaseorAttack + PhysActivity + GenHlth + MentHlth) with logLoss = 0.3306 as our best model for this data set.



```{r}
# Save the best model to a file
saveRDS(logRegFit_1, "logRegFit_1.rds")
```

